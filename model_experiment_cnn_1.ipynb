{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukaJinc/Colab-Pytorch-Kaggle/blob/main/notebooks/loading_kaggle_data_to_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Kaggle data sets directly into Colab**"
      ],
      "metadata": {
        "id": "SyZxTF7lf7jk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlwSaX9akGfG",
        "outputId": "a5842e71-6997-47d2-a00a-cbccdc0e31e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 86% 244M/285M [00:00<00:00, 749MB/s] \n",
            "100% 285M/285M [00:02<00:00, 135MB/s]\n",
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! pip install kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "\n",
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "wandb.init(project=\"facial-expression-recognition\", name=\"robust-cnn-v1\")\n",
        "\n",
        "config = {\n",
        "    \"epochs\": 50,  # Increased epochs for better convergence with augmentation\n",
        "    \"batch_size\": 128, # Can often increase batch size with a better model\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"image_size\": 48,\n",
        "    \"num_classes\": 7,\n",
        "    \"num_workers\": 4 # For faster data loading\n",
        "}\n",
        "wandb.config.update(config)\n",
        "\n",
        "# Data Loading and Efficient Pre-processing\n",
        "# Processing the pixel string every time is slow.\n",
        "# We do it once here to create a new column of numpy arrays.\n",
        "def string_to_array(pixel_string):\n",
        "    return np.array(pixel_string.split(), dtype=np.uint8).reshape(config[\"image_size\"], config[\"image_size\"])\n",
        "\n",
        "# Load the main training data\n",
        "data_path = os.path.expanduser(\"/content/train.csv\") # Assumes data is in home directory\n",
        "if not os.path.exists(data_path):\n",
        "    print(f\"Error: Data file not found at {data_path}\")\n",
        "    print(\"Please download the 'train.csv' from the Kaggle competition and place it in your home directory or update the path.\")\n",
        "    pass\n",
        "\n",
        "full_train_df = pd.read_csv(data_path)\n",
        "full_train_df['pixels_array'] = full_train_df['pixels'].apply(string_to_array)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_df, val_df = train_test_split(\n",
        "    full_train_df,\n",
        "    test_size=0.1,\n",
        "    stratify=full_train_df['emotion'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "class FacialExpressionDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.df = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Directly access the pre-processed numpy array\n",
        "        image_array = self.df.iloc[idx]['pixels_array']\n",
        "        image = Image.fromarray(image_array) # Convert to PIL Image for transforms\n",
        "        label = int(self.df.iloc[idx]['emotion'])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Data Augmentation and Transforms\n",
        "# Augmentation is crucial for this small, noisy dataset.\n",
        "# We create a strong augmentation pipeline for training and a simple one for validation.\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = FacialExpressionDataset(train_df, transform=train_transform)\n",
        "val_dataset = FacialExpressionDataset(val_df, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=config[\"num_workers\"],\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=config[\"num_workers\"],\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Robust Model Architecture\n",
        "# BatchNorm stabilizes training, and AdaptiveAvgPool makes the\n",
        "# classifier robust to input size changes and is more effective than a simple Flatten.\n",
        "class RobustCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(RobustCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 6 * 6, 1024),\n",
        "            nn.BatchNorm1d(1024), nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = RobustCNN(num_classes=config[\"num_classes\"]).to(device)\n",
        "\n",
        "# Loss, Optimizer, and Scheduler\n",
        "# To combat the severe class imbalance in the dataset.\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_df['emotion']),\n",
        "    y=train_df['emotion'].to_numpy()\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "# Using scheduler to automatically reduce the learning rate when validation stops improving.\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "# Training Loop\n",
        "wandb.watch(model, log=\"all\")\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "    model.train()\n",
        "    train_loss, train_correct = 0.0, 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = train_correct / len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "    # Step the scheduler based on validation accuracy\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    # Log metrics to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}, Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
        "\n",
        "    # Save the best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        wandb.save(\"best_model.pth\") # Save best model to W&B\n",
        "        print(f\"New best model saved with validation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "# Finish W&B run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V1HAGpLOVEUQ",
        "outputId": "a9262c3b-385e-4412-eb38-b6c249a3a12f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlchik22\u001b[0m (\u001b[33mlchik22-free-uni\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250607_075412-n1994blf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition/runs/n1994blf' target=\"_blank\">robust-cnn-v1</a></strong> to <a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition' target=\"_blank\">https://wandb.ai/lchik22-free-uni/facial-expression-recognition</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition/runs/n1994blf' target=\"_blank\">https://wandb.ai/lchik22-free-uni/facial-expression-recognition/runs/n1994blf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01: Train Acc=0.2186, Val Acc=0.3002, Train Loss=1.9795, Val Loss=1.7807\n",
            "New best model saved with validation accuracy: 0.3002\n",
            "Epoch 02: Train Acc=0.2982, Val Acc=0.3382, Train Loss=1.7941, Val Loss=1.6008\n",
            "New best model saved with validation accuracy: 0.3382\n",
            "Epoch 03: Train Acc=0.3764, Val Acc=0.4479, Train Loss=1.6299, Val Loss=1.4763\n",
            "New best model saved with validation accuracy: 0.4479\n",
            "Epoch 04: Train Acc=0.4109, Val Acc=0.4709, Train Loss=1.5313, Val Loss=1.4314\n",
            "New best model saved with validation accuracy: 0.4709\n",
            "Epoch 05: Train Acc=0.4393, Val Acc=0.4828, Train Loss=1.4735, Val Loss=1.2984\n",
            "New best model saved with validation accuracy: 0.4828\n",
            "Epoch 06: Train Acc=0.4557, Val Acc=0.5026, Train Loss=1.4345, Val Loss=1.3099\n",
            "New best model saved with validation accuracy: 0.5026\n",
            "Epoch 07: Train Acc=0.4676, Val Acc=0.5378, Train Loss=1.3871, Val Loss=1.2936\n",
            "New best model saved with validation accuracy: 0.5378\n",
            "Epoch 08: Train Acc=0.4795, Val Acc=0.5468, Train Loss=1.3693, Val Loss=1.2305\n",
            "New best model saved with validation accuracy: 0.5468\n",
            "Epoch 09: Train Acc=0.4894, Val Acc=0.5590, Train Loss=1.3448, Val Loss=1.1803\n",
            "New best model saved with validation accuracy: 0.5590\n",
            "Epoch 10: Train Acc=0.4955, Val Acc=0.5427, Train Loss=1.3203, Val Loss=1.1820\n",
            "Epoch 11: Train Acc=0.4990, Val Acc=0.5657, Train Loss=1.2975, Val Loss=1.1587\n",
            "New best model saved with validation accuracy: 0.5657\n",
            "Epoch 12: Train Acc=0.5152, Val Acc=0.5618, Train Loss=1.2701, Val Loss=1.1219\n",
            "Epoch 13: Train Acc=0.5168, Val Acc=0.5688, Train Loss=1.2525, Val Loss=1.1641\n",
            "New best model saved with validation accuracy: 0.5688\n",
            "Epoch 14: Train Acc=0.5272, Val Acc=0.5639, Train Loss=1.2192, Val Loss=1.1432\n",
            "Epoch 15: Train Acc=0.5302, Val Acc=0.5831, Train Loss=1.2247, Val Loss=1.1536\n",
            "New best model saved with validation accuracy: 0.5831\n",
            "Epoch 16: Train Acc=0.5361, Val Acc=0.5601, Train Loss=1.2017, Val Loss=1.1164\n",
            "Epoch 17: Train Acc=0.5364, Val Acc=0.6092, Train Loss=1.1904, Val Loss=1.0896\n",
            "New best model saved with validation accuracy: 0.6092\n",
            "Epoch 18: Train Acc=0.5396, Val Acc=0.6085, Train Loss=1.1888, Val Loss=1.0824\n",
            "Epoch 19: Train Acc=0.5462, Val Acc=0.5994, Train Loss=1.1609, Val Loss=1.1215\n",
            "Epoch 20: Train Acc=0.5527, Val Acc=0.5893, Train Loss=1.1555, Val Loss=1.0795\n",
            "Epoch 21: Train Acc=0.5541, Val Acc=0.5970, Train Loss=1.1459, Val Loss=1.0742\n",
            "Epoch 22: Train Acc=0.5674, Val Acc=0.6151, Train Loss=1.1057, Val Loss=1.0902\n",
            "New best model saved with validation accuracy: 0.6151\n",
            "Epoch 23: Train Acc=0.5690, Val Acc=0.6186, Train Loss=1.0966, Val Loss=1.0291\n",
            "New best model saved with validation accuracy: 0.6186\n",
            "Epoch 24: Train Acc=0.5793, Val Acc=0.6169, Train Loss=1.0775, Val Loss=1.0336\n",
            "Epoch 25: Train Acc=0.5792, Val Acc=0.6116, Train Loss=1.0630, Val Loss=1.0203\n",
            "Epoch 26: Train Acc=0.5817, Val Acc=0.6155, Train Loss=1.0563, Val Loss=1.0072\n",
            "Epoch 27: Train Acc=0.5874, Val Acc=0.6162, Train Loss=1.0443, Val Loss=1.0117\n",
            "Epoch 28: Train Acc=0.5913, Val Acc=0.6242, Train Loss=1.0303, Val Loss=1.0211\n",
            "New best model saved with validation accuracy: 0.6242\n",
            "Epoch 29: Train Acc=0.5986, Val Acc=0.6266, Train Loss=1.0056, Val Loss=0.9914\n",
            "New best model saved with validation accuracy: 0.6266\n",
            "Epoch 30: Train Acc=0.5999, Val Acc=0.6217, Train Loss=1.0034, Val Loss=0.9916\n",
            "Epoch 31: Train Acc=0.6069, Val Acc=0.6287, Train Loss=0.9953, Val Loss=0.9943\n",
            "New best model saved with validation accuracy: 0.6287\n",
            "Epoch 32: Train Acc=0.6046, Val Acc=0.6350, Train Loss=0.9817, Val Loss=0.9970\n",
            "New best model saved with validation accuracy: 0.6350\n",
            "Epoch 33: Train Acc=0.6070, Val Acc=0.6364, Train Loss=0.9827, Val Loss=1.0112\n",
            "New best model saved with validation accuracy: 0.6364\n",
            "Epoch 34: Train Acc=0.6120, Val Acc=0.6343, Train Loss=0.9846, Val Loss=0.9975\n",
            "Epoch 35: Train Acc=0.6071, Val Acc=0.6416, Train Loss=0.9833, Val Loss=0.9871\n",
            "New best model saved with validation accuracy: 0.6416\n",
            "Epoch 36: Train Acc=0.6130, Val Acc=0.6465, Train Loss=0.9731, Val Loss=0.9882\n",
            "New best model saved with validation accuracy: 0.6465\n",
            "Epoch 37: Train Acc=0.6123, Val Acc=0.6454, Train Loss=0.9679, Val Loss=1.0055\n",
            "Epoch 38: Train Acc=0.6098, Val Acc=0.6360, Train Loss=0.9770, Val Loss=0.9928\n",
            "Epoch 39: Train Acc=0.6132, Val Acc=0.6444, Train Loss=0.9669, Val Loss=1.0121\n",
            "Epoch 40: Train Acc=0.6172, Val Acc=0.6440, Train Loss=0.9550, Val Loss=0.9922\n",
            "Epoch 41: Train Acc=0.6204, Val Acc=0.6423, Train Loss=0.9364, Val Loss=0.9783\n",
            "Epoch 42: Train Acc=0.6251, Val Acc=0.6552, Train Loss=0.9355, Val Loss=1.0052\n",
            "New best model saved with validation accuracy: 0.6552\n",
            "Epoch 43: Train Acc=0.6258, Val Acc=0.6524, Train Loss=0.9350, Val Loss=0.9933\n",
            "Epoch 44: Train Acc=0.6265, Val Acc=0.6520, Train Loss=0.9274, Val Loss=0.9980\n",
            "Epoch 45: Train Acc=0.6242, Val Acc=0.6496, Train Loss=0.9341, Val Loss=1.0002\n",
            "Epoch 46: Train Acc=0.6281, Val Acc=0.6517, Train Loss=0.9303, Val Loss=1.0050\n",
            "Epoch 47: Train Acc=0.6305, Val Acc=0.6489, Train Loss=0.9113, Val Loss=0.9958\n",
            "Epoch 48: Train Acc=0.6317, Val Acc=0.6506, Train Loss=0.9122, Val Loss=0.9934\n",
            "Epoch 49: Train Acc=0.6283, Val Acc=0.6493, Train Loss=0.9145, Val Loss=0.9867\n",
            "Epoch 50: Train Acc=0.6328, Val Acc=0.6555, Train Loss=0.9101, Val Loss=1.0035\n",
            "New best model saved with validation accuracy: 0.6555\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>████████████████▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▄▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▄▄▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>val_loss</td><td>█▆▅▅▄▃▃▃▃▂▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>learning_rate</td><td>6e-05</td></tr><tr><td>train_accuracy</td><td>0.63275</td></tr><tr><td>train_loss</td><td>0.91012</td></tr><tr><td>val_accuracy</td><td>0.65552</td></tr><tr><td>val_loss</td><td>1.00354</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">robust-cnn-v1</strong> at: <a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition/runs/n1994blf' target=\"_blank\">https://wandb.ai/lchik22-free-uni/facial-expression-recognition/runs/n1994blf</a><br> View project at: <a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition' target=\"_blank\">https://wandb.ai/lchik22-free-uni/facial-expression-recognition</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250607_075412-n1994blf/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
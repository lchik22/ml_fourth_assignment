{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g_hFJlspCOc",
        "outputId": "d3b9917e-8a1f-4fe8-f044-c1bd2a1ef609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "challenges-in-representation-learning-facial-expression-recognition-challenge.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "replace example_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! pip install kaggle\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "\n",
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models # We now import models from torchvision\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Initialize W&B and Configuration\n",
        "wandb.init(project=\"facial-expression-recognition\", name=\"dream-model-resnet18-finetune-v1.0\")\n",
        "\n",
        "config = {\n",
        "    \"head_only_epochs\": 5,   # Epochs for Phase 1 (training the head)\n",
        "    \"full_tune_epochs\": 20, # Epochs for Phase 2 (fine-tuning all layers)\n",
        "    \"batch_size\": 128,      # May need to reduce if I get memory errors\n",
        "    \"head_lr\": 1e-3,        # Learning rate for the new head\n",
        "    \"full_tune_lr\": 1e-5,   # Very low learning rate for fine-tuning\n",
        "    \"image_size\": 224,      # I will resize images to what ResNet expects\n",
        "    \"num_classes\": 7,\n",
        "    \"num_workers\": 2\n",
        "}\n",
        "wandb.config.update(config)\n",
        "\n",
        "# Data Loading and Efficient Pre-processing\n",
        "def string_to_array(pixel_string):\n",
        "    # The original images are 48x48\n",
        "    return np.array(pixel_string.split(), dtype=np.uint8).reshape(48, 48)\n",
        "\n",
        "data_path = os.path.expanduser(\"/content/train.csv\")\n",
        "if not os.path.exists(data_path):\n",
        "    print(f\"Error: Data file not found at {data_path}\")\n",
        "    pass\n",
        "\n",
        "full_train_df = pd.read_csv(data_path)\n",
        "full_train_df['pixels_array'] = full_train_df['pixels'].apply(string_to_array)\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    full_train_df, test_size=0.1, stratify=full_train_df['emotion'], random_state=42\n",
        ")\n",
        "\n",
        "# Transforms for Transfer Learning\n",
        "# This is a CRITICAL step for adapting data to the pre-trained model\n",
        "# ImageNet normalization stats\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # ResNet expects 3-channel RGB images. I repeat the grayscale channel 3 times.\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    # Resize images to the size expected by ResNet\n",
        "    transforms.Resize(config[\"image_size\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std), # Use ImageNet stats\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1))\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize(config[\"image_size\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
        "])\n",
        "\n",
        "# Dataset and DataLoader\n",
        "class FacialExpressionDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.df = dataframe\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        image_array = self.df.iloc[idx]['pixels_array']\n",
        "        image = Image.fromarray(image_array)\n",
        "        label = int(self.df.iloc[idx]['emotion'])\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "train_dataset = FacialExpressionDataset(train_df, transform=train_transform)\n",
        "val_dataset = FacialExpressionDataset(val_df, transform=val_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"], pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"], pin_memory=True)\n",
        "\n",
        "# The Dream Model: Loading and Modifying ResNet18\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pre-trained ResNet18\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# PHASE 1 SETUP: FREEZE PRE-TRAINED LAYERS\n",
        "print(\"--- Phase 1: Training the classifier head ---\")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False # Freeze all layers initially\n",
        "\n",
        "# Replace the final fully connected layer with my own\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, config[\"num_classes\"])\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss, Optimizer for Phase 1\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['emotion']), y=train_df['emotion'].to_numpy())\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# I only optimize the parameters of the new head\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=config[\"head_lr\"])\n",
        "\n",
        "# Training Loop (Combined for both phases)\n",
        "wandb.watch(model, log=\"all\", log_freq=100)\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for phase in [\"head_only\", \"full_tune\"]:\n",
        "    if phase == \"full_tune\":\n",
        "        print(\"\\n--- Phase 2: Fine-tuning all layers ---\")\n",
        "        # Unfreeze all layers\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "        # Use a very low learning rate for all layers\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"full_tune_lr\"])\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "        epochs = config[\"full_tune_epochs\"]\n",
        "    else: # phase == \"head_only\"\n",
        "        epochs = config[\"head_only_epochs\"]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0.0, 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss, val_correct = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = train_correct / len(train_loader.dataset)\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        if phase == \"full_tune\":\n",
        "            scheduler.step(val_acc)\n",
        "\n",
        "        wandb.log({\n",
        "            \"phase\": 1 if phase == \"head_only\" else 2,\n",
        "            \"epoch\": epoch + 1, \"train_loss\": train_loss, \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss, \"val_accuracy\": val_acc, \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "        })\n",
        "\n",
        "        print(f\"Phase '{phase}' - Epoch {epoch+1:02d}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"dream_model_best.pth\")\n",
        "            wandb.save(\"dream_model_best.pth\")\n",
        "            print(f\"New best model saved with validation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lBwqag1IpzKN",
        "outputId": "b790576b-b7a7-4cd0-e5e6-6672889d18dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlchik22\u001b[0m (\u001b[33mlchik22-free-uni\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250607_094135-m8jsutl6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition/runs/m8jsutl6' target=\"_blank\">dream-model-resnet18-finetune-v1.0</a></strong> to <a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition' target=\"_blank\">https://wandb.ai/lchik22-free-uni/facial-expression-recognition</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition/runs/m8jsutl6' target=\"_blank\">https://wandb.ai/lchik22-free-uni/facial-expression-recognition/runs/m8jsutl6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 1: Training the classifier head ---\n",
            "Phase 'head_only' - Epoch 01: Train Acc=0.2510, Val Acc=0.3204\n",
            "New best model saved with validation accuracy: 0.3204\n",
            "Phase 'head_only' - Epoch 02: Train Acc=0.3159, Val Acc=0.3971\n",
            "New best model saved with validation accuracy: 0.3971\n",
            "Phase 'head_only' - Epoch 03: Train Acc=0.3400, Val Acc=0.3845\n",
            "Phase 'head_only' - Epoch 04: Train Acc=0.3436, Val Acc=0.3292\n",
            "Phase 'head_only' - Epoch 05: Train Acc=0.3503, Val Acc=0.3842\n",
            "\n",
            "--- Phase 2: Fine-tuning all layers ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 'full_tune' - Epoch 01: Train Acc=0.3970, Val Acc=0.4591\n",
            "New best model saved with validation accuracy: 0.4591\n",
            "Phase 'full_tune' - Epoch 02: Train Acc=0.4665, Val Acc=0.4936\n",
            "New best model saved with validation accuracy: 0.4936\n",
            "Phase 'full_tune' - Epoch 03: Train Acc=0.4964, Val Acc=0.5158\n",
            "New best model saved with validation accuracy: 0.5158\n",
            "Phase 'full_tune' - Epoch 04: Train Acc=0.5229, Val Acc=0.5381\n",
            "New best model saved with validation accuracy: 0.5381\n",
            "Phase 'full_tune' - Epoch 05: Train Acc=0.5464, Val Acc=0.5583\n",
            "New best model saved with validation accuracy: 0.5583\n",
            "Phase 'full_tune' - Epoch 06: Train Acc=0.5598, Val Acc=0.5681\n",
            "New best model saved with validation accuracy: 0.5681\n",
            "Phase 'full_tune' - Epoch 07: Train Acc=0.5804, Val Acc=0.5803\n",
            "New best model saved with validation accuracy: 0.5803\n",
            "Phase 'full_tune' - Epoch 08: Train Acc=0.5881, Val Acc=0.5817\n",
            "New best model saved with validation accuracy: 0.5817\n",
            "Phase 'full_tune' - Epoch 09: Train Acc=0.6035, Val Acc=0.5879\n",
            "New best model saved with validation accuracy: 0.5879\n",
            "Phase 'full_tune' - Epoch 10: Train Acc=0.6100, Val Acc=0.5994\n",
            "New best model saved with validation accuracy: 0.5994\n",
            "Phase 'full_tune' - Epoch 11: Train Acc=0.6209, Val Acc=0.6092\n",
            "New best model saved with validation accuracy: 0.6092\n",
            "Phase 'full_tune' - Epoch 12: Train Acc=0.6281, Val Acc=0.6113\n",
            "New best model saved with validation accuracy: 0.6113\n",
            "Phase 'full_tune' - Epoch 13: Train Acc=0.6369, Val Acc=0.6144\n",
            "New best model saved with validation accuracy: 0.6144\n",
            "Phase 'full_tune' - Epoch 14: Train Acc=0.6446, Val Acc=0.6210\n",
            "New best model saved with validation accuracy: 0.6210\n",
            "Phase 'full_tune' - Epoch 15: Train Acc=0.6477, Val Acc=0.6287\n",
            "New best model saved with validation accuracy: 0.6287\n",
            "Phase 'full_tune' - Epoch 16: Train Acc=0.6567, Val Acc=0.6266\n",
            "Phase 'full_tune' - Epoch 17: Train Acc=0.6673, Val Acc=0.6259\n",
            "Phase 'full_tune' - Epoch 18: Train Acc=0.6725, Val Acc=0.6367\n",
            "New best model saved with validation accuracy: 0.6367\n",
            "Phase 'full_tune' - Epoch 19: Train Acc=0.6767, Val Acc=0.6357\n",
            "Phase 'full_tune' - Epoch 20: Train Acc=0.6796, Val Acc=0.6426\n",
            "New best model saved with validation accuracy: 0.6426\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>learning_rate</td><td>█████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>phase</td><td>▁▁▁▁▁████████████████████</td></tr><tr><td>train_accuracy</td><td>▁▂▂▃▃▃▅▅▅▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▂▁▂▄▅▅▆▆▆▇▇▇▇▇▇▇███████</td></tr><tr><td>val_loss</td><td>██▇▇▇▅▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>phase</td><td>2</td></tr><tr><td>train_accuracy</td><td>0.67962</td></tr><tr><td>train_loss</td><td>0.79585</td></tr><tr><td>val_accuracy</td><td>0.64263</td></tr><tr><td>val_loss</td><td>0.98752</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dream-model-resnet18-finetune-v1.0</strong> at: <a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition/runs/m8jsutl6' target=\"_blank\">https://wandb.ai/lchik22-free-uni/facial-expression-recognition/runs/m8jsutl6</a><br> View project at: <a href='https://wandb.ai/lchik22-free-uni/facial-expression-recognition' target=\"_blank\">https://wandb.ai/lchik22-free-uni/facial-expression-recognition</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250607_094135-m8jsutl6/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}